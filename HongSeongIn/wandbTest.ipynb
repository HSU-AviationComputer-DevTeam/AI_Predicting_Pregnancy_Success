{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, RandomizedSearchCV\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n",
      "File \u001b[1;32mc:\\Users\\tjddl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\__init__.py:79\u001b[0m\n\u001b[0;32m     68\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of sklearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     80\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "File \u001b[1;32mc:\\Users\\tjddl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\__check_build\\__init__.py:45\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124m___________________________________________________________________________\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124mContents of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124m`make` in the source directory.\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e, local_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(dir_content)\u001b[38;5;241m.\u001b[39mstrip(), msg))\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_check_build\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_build  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     47\u001b[0m     raise_build_error(e)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:404\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# 🟢 WandB 프로젝트 초기화\n",
    "wandb.init(project=\"xgboost-hyperparam-tuning\", name=\"xgb_experiment\")\n",
    "\n",
    "# 문자열 → 숫자 변환 함수\n",
    "def convert_count_str(val):\n",
    "    if pd.isna(val):\n",
    "        return 0\n",
    "    val = str(val).strip()\n",
    "    if \"회 이상\" in val:\n",
    "        return 6\n",
    "    m = re.search(r'(\\d+)회?', val)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    return 0\n",
    "\n",
    "# 정자/난자 기증자 나이 맵핑\n",
    "donor_age_mapping = {\n",
    "    '만20세 이하': 3, '만21-25세': 5, '만26-30세': 4, '만31-35세': 2,\n",
    "    '만36-40세': 1, '만41-45세': 0, '알 수 없음': 0\n",
    "}\n",
    "\n",
    "def convert_donor_age(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    return donor_age_mapping.get(str(val).strip(), np.nan)\n",
    "\n",
    "# 카테고리형 변수 NaN 처리\n",
    "def convert_nan_to_string(df, category_columns):\n",
    "    df_copy = df.copy()\n",
    "    for col in category_columns:\n",
    "        df_copy[col] = df_copy[col].fillna('NaN')\n",
    "    return df_copy\n",
    "\n",
    "#  1. 데이터 로드 및 전처리\n",
    "train = pd.read_csv('train.csv').drop(columns=['ID'])\n",
    "test = pd.read_csv('test.csv').drop(columns=['ID'])\n",
    "\n",
    "# 가중치 데이터 로드\n",
    "weight_data = pd.read_csv('og_weighted_hong.csv', encoding='utf-8')\n",
    "weight_dict = weight_data.set_index(\"데이터 항목\").to_dict()\n",
    "\n",
    "# '시술 당시 나이' 변환\n",
    "age_mapping = {\n",
    "    '만18-34세': 5, '만35-37세': 4, '만38-39세': 3, '만40-42세': 2, '만43-44세': 1, '만45-50세': 0, '알 수 없음': np.nan\n",
    "}\n",
    "train['시술 당시 나이'] = train['시술 당시 나이'].apply(lambda x: float(age_mapping.get(str(x).strip(), 0)))\n",
    "test['시술 당시 나이'] = test['시술 당시 나이'].apply(lambda x: float(age_mapping.get(str(x).strip(), 0)))\n",
    "\n",
    "# 횟수 관련 컬럼 변환\n",
    "count_columns = [\"총 시술 횟수\", \"클리닉 내 총 시술 횟수\", \"IVF 시술 횟수\", \"DI 시술 횟수\",\n",
    "                 \"총 임신 횟수\", \"IVF 임신 횟수\", \"DI 임신 횟수\", \"총 출산 횟수\", \"IVF 출산 횟수\", \"DI 출산 횟수\"]\n",
    "for col in count_columns:\n",
    "    train[col] = train[col].astype(str).apply(convert_count_str).astype(int)\n",
    "    test[col] = test[col].astype(str).apply(convert_count_str).astype(int)\n",
    "\n",
    "# 난자/정자 기증자 나이 변환\n",
    "train['난자 기증자 나이'] = train['난자 기증자 나이'].astype(str).apply(convert_donor_age)\n",
    "test['난자 기증자 나이'] = test['난자 기증자 나이'].astype(str).apply(convert_donor_age)\n",
    "train['정자 기증자 나이'] = train['정자 기증자 나이'].astype(str).apply(convert_donor_age)\n",
    "test['정자 기증자 나이'] = test['정자 기증자 나이'].astype(str).apply(convert_donor_age)\n",
    "\n",
    "# Feature 가중치 적용\n",
    "def apply_feature_weights(X, weight_dict):\n",
    "    X_weighted = X.copy()\n",
    "    for column in X.columns:\n",
    "        if column in weight_dict[\"IVF\"]:\n",
    "            X_weighted[column] *= weight_dict[\"IVF\"][column]  # IVF 가중치 적용\n",
    "    return X_weighted\n",
    "\n",
    "X = train.drop('임신 성공 여부', axis=1)\n",
    "y = train['임신 성공 여부']\n",
    "X_weighted = apply_feature_weights(X, weight_dict)\n",
    "X_test_weighted = apply_feature_weights(test, weight_dict)\n",
    "\n",
    "# 데이터 불균형 처리\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "X_resampled, y_resampled = undersample.fit_resample(X_weighted, y)\n",
    "\n",
    "# 데이터 타입 변환\n",
    "category_columns = [\"시술 시기 코드\", \"시술 유형\", \"특정 시술 유형\", \"배란 유도 유형\", \"배아 생성 주요 이유\", \"난자 출처\", \"정자 출처\"]\n",
    "X_resampled = convert_nan_to_string(X_resampled, category_columns)\n",
    "X_test_weighted = convert_nan_to_string(X_test_weighted, category_columns)\n",
    "\n",
    "# 학습/검증 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "# Stacking 모델 설정\n",
    "stack_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', XGBClassifier(tree_method='gpu_hist', enable_categorical=True, random_state=42)),\n",
    "        ('lgbm', LGBMClassifier(n_jobs=-2, random_state=42, verbose=-1)),\n",
    "        ('cat', CatBoostClassifier(task_type='GPU', verbose=0))\n",
    "    ],\n",
    "    final_estimator=Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('lr', LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear'))\n",
    "    ]),\n",
    "    cv=3,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# RandomizedSearchCV를 WandB와 함께 실행\n",
    "param_dist = {\n",
    "    'xgb__n_estimators': [300, 400, 500],\n",
    "    'xgb__max_depth': [4, 5, 6],\n",
    "    'xgb__learning_rate': [0.025, 0.05, 0.1],\n",
    "    'final_estimator__lr__C': [0.1, 1.0, 5.0, 10.0]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(stack_clf, param_distributions=param_dist, n_iter=10, scoring='roc_auc', cv=3, n_jobs=1, random_state=42, verbose=2)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 결과 로깅\n",
    "wandb.log({\"Best ROC AUC\": random_search.best_score_})\n",
    "print(f\"Best Validation ROC AUC: {random_search.best_score_:.5f}\")\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "pred_proba = random_search.best_estimator_.predict_proba(X_test_weighted)[:, 1]\n",
    "submission = pd.DataFrame({'ID': [f\"TEST_{i:05d}\" for i in range(len(test))], 'probability': pred_proba})\n",
    "submission.to_csv('Result_submit.csv', index=False)\n",
    "print(\"제출 파일 생성 완료\")\n",
    "\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
