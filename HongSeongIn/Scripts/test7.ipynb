{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Fold 1/5 ====\n",
      "Training CatBoost...\n",
      "0:\ttest: 0.7107090\tbest: 0.7107090 (0)\ttotal: 616ms\tremaining: 15m 22s\n",
      "200:\ttest: 0.7353929\tbest: 0.7353929 (200)\ttotal: 1m 44s\tremaining: 11m 14s\n",
      "400:\ttest: 0.7364822\tbest: 0.7364962 (391)\ttotal: 3m 1s\tremaining: 8m 17s\n",
      "600:\ttest: 0.7367941\tbest: 0.7367941 (600)\ttotal: 4m 15s\tremaining: 6m 22s\n",
      "800:\ttest: 0.7370653\tbest: 0.7370789 (794)\ttotal: 6m\tremaining: 5m 14s\n",
      "1000:\ttest: 0.7371801\tbest: 0.7371801 (1000)\ttotal: 8m 14s\tremaining: 4m 6s\n",
      "1200:\ttest: 0.7371940\tbest: 0.7371963 (1197)\ttotal: 10m 23s\tremaining: 2m 35s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7372078237\n",
      "bestIteration = 1234\n",
      "\n",
      "Shrink model to first 1235 iterations.\n",
      "Training LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tjddl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\callback.py:329: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning(\"Early stopping is not available in dart mode\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Fold 1 AUC: 0.737208\n",
      "LightGBM Fold 1 AUC: 0.736840\n",
      "Ensemble Fold 1 AUC: 0.737999 (weights: [0.50006438 0.49993562])\n",
      "New best model found! Score: 0.737999\n",
      "훈련 데이터 크기: 205080 -> 리샘플링 후: 158946\n",
      "훈련 데이터 클래스 분포: [152098  52982]\n",
      "리샘플링 후 클래스 분포: [105964  52982]\n",
      "\n",
      "==== Fold 2/5 ====\n",
      "Training CatBoost...\n",
      "0:\ttest: 0.7216198\tbest: 0.7216198 (0)\ttotal: 552ms\tremaining: 13m 48s\n",
      "200:\ttest: 0.7393168\tbest: 0.7393168 (200)\ttotal: 1m 45s\tremaining: 11m 23s\n",
      "400:\ttest: 0.7406804\tbest: 0.7406804 (400)\ttotal: 3m 11s\tremaining: 8m 44s\n",
      "600:\ttest: 0.7412550\tbest: 0.7412550 (600)\ttotal: 4m 28s\tremaining: 6m 42s\n",
      "800:\ttest: 0.7419574\tbest: 0.7419574 (800)\ttotal: 6m 28s\tremaining: 5m 39s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7419910338\n",
      "bestIteration = 843\n",
      "\n",
      "Shrink model to first 844 iterations.\n",
      "Training LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tjddl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\callback.py:329: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning(\"Early stopping is not available in dart mode\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Fold 2 AUC: 0.741991\n",
      "LightGBM Fold 2 AUC: 0.742073\n",
      "Ensemble Fold 2 AUC: 0.742853 (weights: [0.50003903 0.49996097])\n",
      "New best model found! Score: 0.742853\n",
      "훈련 데이터 크기: 205081 -> 리샘플링 후: 158949\n",
      "훈련 데이터 클래스 분포: [152098  52983]\n",
      "리샘플링 후 클래스 분포: [105966  52983]\n",
      "\n",
      "==== Fold 3/5 ====\n",
      "Training CatBoost...\n",
      "0:\ttest: 0.7217223\tbest: 0.7217223 (0)\ttotal: 583ms\tremaining: 14m 33s\n",
      "200:\ttest: 0.7369141\tbest: 0.7369141 (200)\ttotal: 1m 55s\tremaining: 12m 27s\n",
      "400:\ttest: 0.7377982\tbest: 0.7377982 (400)\ttotal: 3m 33s\tremaining: 9m 44s\n",
      "600:\ttest: 0.7382232\tbest: 0.7382232 (600)\ttotal: 5m 9s\tremaining: 7m 42s\n",
      "800:\ttest: 0.7387559\tbest: 0.7387637 (788)\ttotal: 7m 22s\tremaining: 6m 26s\n",
      "1000:\ttest: 0.7389259\tbest: 0.7389259 (1000)\ttotal: 9m 44s\tremaining: 4m 51s\n",
      "1200:\ttest: 0.7389794\tbest: 0.7389794 (1199)\ttotal: 12m 1s\tremaining: 2m 59s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7389910881\n",
      "bestIteration = 1283\n",
      "\n",
      "Shrink model to first 1284 iterations.\n",
      "Training LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tjddl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\callback.py:329: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning(\"Early stopping is not available in dart mode\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Fold 3 AUC: 0.738991\n",
      "LightGBM Fold 3 AUC: 0.740003\n",
      "Ensemble Fold 3 AUC: 0.740412 (weights: [0.49965797 0.50034203])\n",
      "훈련 데이터 크기: 205081 -> 리샘플링 후: 158949\n",
      "훈련 데이터 클래스 분포: [152098  52983]\n",
      "리샘플링 후 클래스 분포: [105966  52983]\n",
      "\n",
      "==== Fold 4/5 ====\n",
      "Training CatBoost...\n",
      "0:\ttest: 0.7147980\tbest: 0.7147980 (0)\ttotal: 622ms\tremaining: 15m 31s\n",
      "200:\ttest: 0.7360212\tbest: 0.7360212 (200)\ttotal: 2m 6s\tremaining: 13m 36s\n",
      "400:\ttest: 0.7366896\tbest: 0.7366896 (400)\ttotal: 4m\tremaining: 10m 57s\n",
      "600:\ttest: 0.7368259\tbest: 0.7368259 (600)\ttotal: 5m 46s\tremaining: 8m 38s\n",
      "800:\ttest: 0.7369912\tbest: 0.7370373 (724)\ttotal: 8m 11s\tremaining: 7m 8s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.737037306\n",
      "bestIteration = 724\n",
      "\n",
      "Shrink model to first 725 iterations.\n",
      "Training LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tjddl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\callback.py:329: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning(\"Early stopping is not available in dart mode\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Fold 4 AUC: 0.737037\n",
      "LightGBM Fold 4 AUC: 0.737128\n",
      "Ensemble Fold 4 AUC: 0.738024 (weights: [0.49984195 0.50015805])\n",
      "훈련 데이터 크기: 205081 -> 리샘플링 후: 158946\n",
      "훈련 데이터 클래스 분포: [152099  52982]\n",
      "리샘플링 후 클래스 분포: [105964  52982]\n",
      "\n",
      "==== Fold 5/5 ====\n",
      "Training CatBoost...\n",
      "0:\ttest: 0.7125183\tbest: 0.7125183 (0)\ttotal: 665ms\tremaining: 16m 37s\n",
      "200:\ttest: 0.7365073\tbest: 0.7365073 (200)\ttotal: 2m 2s\tremaining: 13m 14s\n",
      "400:\ttest: 0.7378688\tbest: 0.7378700 (395)\ttotal: 3m 52s\tremaining: 10m 38s\n",
      "600:\ttest: 0.7383365\tbest: 0.7383415 (598)\ttotal: 5m 39s\tremaining: 8m 27s\n",
      "800:\ttest: 0.7387398\tbest: 0.7387486 (792)\ttotal: 7m 57s\tremaining: 6m 56s\n",
      "1000:\ttest: 0.7388860\tbest: 0.7388860 (1000)\ttotal: 9m 53s\tremaining: 4m 56s\n",
      "1200:\ttest: 0.7389353\tbest: 0.7389609 (1169)\ttotal: 11m 48s\tremaining: 2m 56s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7389754459\n",
      "bestIteration = 1267\n",
      "\n",
      "Shrink model to first 1268 iterations.\n",
      "Training LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tjddl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\callback.py:329: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning(\"Early stopping is not available in dart mode\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Fold 5 AUC: 0.738975\n",
      "LightGBM Fold 5 AUC: 0.740550\n",
      "Ensemble Fold 5 AUC: 0.740773 (weights: [0.49946782 0.50053218])\n",
      "훈련 데이터 크기: 205081 -> 리샘플링 후: 158946\n",
      "훈련 데이터 클래스 분포: [152099  52982]\n",
      "리샘플링 후 클래스 분포: [105964  52982]\n",
      "\n",
      "CatBoost OOF AUC: 0.738812\n",
      "LightGBM OOF AUC: 0.739308\n",
      "Final Ensemble OOF AUC: 0.739997\n",
      "Final weights: CatBoost=0.4998, LightGBM=0.5002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 52509 (\\N{HANGUL SYLLABLE CONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 49373 (\\N{HANGUL SYLLABLE SAENG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 49457 (\\N{HANGUL SYLLABLE SEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 48176 (\\N{HANGUL SYLLABLE BAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 50500 (\\N{HANGUL SYLLABLE A}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 49884 (\\N{HANGUL SYLLABLE SI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 49696 (\\N{HANGUL SYLLABLE SUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 45817 (\\N{HANGUL SYLLABLE DANG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 45208 (\\N{HANGUL SYLLABLE NA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 51665 (\\N{HANGUL SYLLABLE JIB}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 46108 (\\N{HANGUL SYLLABLE DOEN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 49888 (\\N{HANGUL SYLLABLE SIN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 49440 (\\N{HANGUL SYLLABLE SEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 45212 (\\N{HANGUL SYLLABLE NAN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 51088 (\\N{HANGUL SYLLABLE JA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 51200 (\\N{HANGUL SYLLABLE JEO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 51109 (\\N{HANGUL SYLLABLE JANG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 49885 (\\N{HANGUL SYLLABLE SIG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 44592 (\\N{HANGUL SYLLABLE GI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 53076 (\\N{HANGUL SYLLABLE KO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 46300 (\\N{HANGUL SYLLABLE DEU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 44221 (\\N{HANGUL SYLLABLE GYEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 44284 (\\N{HANGUL SYLLABLE GWA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 51068 (\\N{HANGUL SYLLABLE IL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 54844 (\\N{HANGUL SYLLABLE HON}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 54633 (\\N{HANGUL SYLLABLE HAB}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 54028 (\\N{HANGUL SYLLABLE PA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 53944 (\\N{HANGUL SYLLABLE TEU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 45320 (\\N{HANGUL SYLLABLE NEO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 50752 (\\N{HANGUL SYLLABLE WA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 48120 (\\N{HANGUL SYLLABLE MI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 49464 (\\N{HANGUL SYLLABLE SE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 51452 (\\N{HANGUL SYLLABLE JU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 51077 (\\N{HANGUL SYLLABLE IB}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 50640 (\\N{HANGUL SYLLABLE E}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 49436 (\\N{HANGUL SYLLABLE SEO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 54943 (\\N{HANGUL SYLLABLE HOES}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 53364 (\\N{HANGUL SYLLABLE KEUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 47532 (\\N{HANGUL SYLLABLE RI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 45769 (\\N{HANGUL SYLLABLE NIG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 45236 (\\N{HANGUL SYLLABLE NAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 51076 (\\N{HANGUL SYLLABLE IM}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 54644 (\\N{HANGUL SYLLABLE HAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 46041 (\\N{HANGUL SYLLABLE DONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 52636 (\\N{HANGUL SYLLABLE CUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 49328 (\\N{HANGUL SYLLABLE SAN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:323: UserWarning: Glyph 51613 (\\N{HANGUL SYLLABLE JEUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 52509 (\\N{HANGUL SYLLABLE CONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 49373 (\\N{HANGUL SYLLABLE SAENG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 49457 (\\N{HANGUL SYLLABLE SEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 48176 (\\N{HANGUL SYLLABLE BAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 50500 (\\N{HANGUL SYLLABLE A}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 49884 (\\N{HANGUL SYLLABLE SI}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 49696 (\\N{HANGUL SYLLABLE SUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 45817 (\\N{HANGUL SYLLABLE DANG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 45208 (\\N{HANGUL SYLLABLE NA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 51665 (\\N{HANGUL SYLLABLE JIB}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 46108 (\\N{HANGUL SYLLABLE DOEN}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 49888 (\\N{HANGUL SYLLABLE SIN}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 49440 (\\N{HANGUL SYLLABLE SEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 45212 (\\N{HANGUL SYLLABLE NAN}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 51088 (\\N{HANGUL SYLLABLE JA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 51200 (\\N{HANGUL SYLLABLE JEO}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 51109 (\\N{HANGUL SYLLABLE JANG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 49885 (\\N{HANGUL SYLLABLE SIG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 44592 (\\N{HANGUL SYLLABLE GI}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 53076 (\\N{HANGUL SYLLABLE KO}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 46300 (\\N{HANGUL SYLLABLE DEU}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 44221 (\\N{HANGUL SYLLABLE GYEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 44284 (\\N{HANGUL SYLLABLE GWA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 51068 (\\N{HANGUL SYLLABLE IL}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 54844 (\\N{HANGUL SYLLABLE HON}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 54633 (\\N{HANGUL SYLLABLE HAB}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 54028 (\\N{HANGUL SYLLABLE PA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 53944 (\\N{HANGUL SYLLABLE TEU}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 45320 (\\N{HANGUL SYLLABLE NEO}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 50752 (\\N{HANGUL SYLLABLE WA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 48120 (\\N{HANGUL SYLLABLE MI}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 49464 (\\N{HANGUL SYLLABLE SE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 51452 (\\N{HANGUL SYLLABLE JU}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 51077 (\\N{HANGUL SYLLABLE IB}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 50640 (\\N{HANGUL SYLLABLE E}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 49436 (\\N{HANGUL SYLLABLE SEO}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 54943 (\\N{HANGUL SYLLABLE HOES}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 53364 (\\N{HANGUL SYLLABLE KEUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 47532 (\\N{HANGUL SYLLABLE RI}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 45769 (\\N{HANGUL SYLLABLE NIG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 45236 (\\N{HANGUL SYLLABLE NAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 51076 (\\N{HANGUL SYLLABLE IM}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 54644 (\\N{HANGUL SYLLABLE HAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 46041 (\\N{HANGUL SYLLABLE DONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 52636 (\\N{HANGUL SYLLABLE CUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 49328 (\\N{HANGUL SYLLABLE SAN}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_22200\\208769085.py:324: UserWarning: Glyph 51613 (\\N{HANGUL SYLLABLE JEUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance plot saved.\n",
      "\n",
      "Submission saved: cat_lgb_ensemble.csv\n",
      "Final Ensemble OOF AUC: 0.739997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 52509 (\\N{HANGUL SYLLABLE CONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49373 (\\N{HANGUL SYLLABLE SAENG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49457 (\\N{HANGUL SYLLABLE SEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 48176 (\\N{HANGUL SYLLABLE BAE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50500 (\\N{HANGUL SYLLABLE A}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49884 (\\N{HANGUL SYLLABLE SI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49696 (\\N{HANGUL SYLLABLE SUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45817 (\\N{HANGUL SYLLABLE DANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45208 (\\N{HANGUL SYLLABLE NA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51665 (\\N{HANGUL SYLLABLE JIB}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 46108 (\\N{HANGUL SYLLABLE DOEN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49888 (\\N{HANGUL SYLLABLE SIN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49440 (\\N{HANGUL SYLLABLE SEON}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45212 (\\N{HANGUL SYLLABLE NAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51088 (\\N{HANGUL SYLLABLE JA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51200 (\\N{HANGUL SYLLABLE JEO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51109 (\\N{HANGUL SYLLABLE JANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49885 (\\N{HANGUL SYLLABLE SIG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44592 (\\N{HANGUL SYLLABLE GI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 53076 (\\N{HANGUL SYLLABLE KO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 46300 (\\N{HANGUL SYLLABLE DEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44221 (\\N{HANGUL SYLLABLE GYEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44284 (\\N{HANGUL SYLLABLE GWA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51068 (\\N{HANGUL SYLLABLE IL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54844 (\\N{HANGUL SYLLABLE HON}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54633 (\\N{HANGUL SYLLABLE HAB}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54028 (\\N{HANGUL SYLLABLE PA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 53944 (\\N{HANGUL SYLLABLE TEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45320 (\\N{HANGUL SYLLABLE NEO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50752 (\\N{HANGUL SYLLABLE WA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 48120 (\\N{HANGUL SYLLABLE MI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49464 (\\N{HANGUL SYLLABLE SE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51452 (\\N{HANGUL SYLLABLE JU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51077 (\\N{HANGUL SYLLABLE IB}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50640 (\\N{HANGUL SYLLABLE E}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49436 (\\N{HANGUL SYLLABLE SEO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54943 (\\N{HANGUL SYLLABLE HOES}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 53364 (\\N{HANGUL SYLLABLE KEUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 47532 (\\N{HANGUL SYLLABLE RI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45769 (\\N{HANGUL SYLLABLE NIG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45236 (\\N{HANGUL SYLLABLE NAE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51076 (\\N{HANGUL SYLLABLE IM}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54644 (\\N{HANGUL SYLLABLE HAE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 46041 (\\N{HANGUL SYLLABLE DONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 52636 (\\N{HANGUL SYLLABLE CUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49328 (\\N{HANGUL SYLLABLE SAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51613 (\\N{HANGUL SYLLABLE JEUNG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pandas >= 1.3.0\n",
    "# numpy >= 1.20.0\n",
    "# matplotlib >= 3.4.0\n",
    "# seaborn >= 0.11.0\n",
    "# scikit-learn >= 1.0.0\n",
    "# imbalanced-learn >= 0.8.0\n",
    "# catboost >= 1.0.0\n",
    "# lightgbm >= 3.3.0\n",
    "# scipy >= 1.7.0\n",
    "# xgboost >= 1.7.0\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, PowerTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 데이터 불러오기 및 전처리\n",
    "def preprocess_data(train_path, test_path):\n",
    "    print(\"Loading data...\")\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    sample_submission = pd.read_csv(test_path.replace('test.csv', 'sample_submission.csv'))\n",
    "    \n",
    "    # ID 컬럼 제거\n",
    "    if 'ID' in train.columns:\n",
    "        train.drop(columns=['ID'], inplace=True)\n",
    "    if 'ID' in test.columns:\n",
    "        test.drop(columns=['ID'], inplace=True)\n",
    "\n",
    "    y = train['임신 성공 여부']\n",
    "    X = train.drop(columns=['임신 성공 여부'])\n",
    "    X_test = test.copy()\n",
    "\n",
    "    # 결측치 처리\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            X[col] = X[col].fillna('Unknown')\n",
    "            X_test[col] = X_test[col].fillna('Unknown')\n",
    "        else:\n",
    "            X[col] = X[col].fillna(X[col].mean())\n",
    "            X_test[col] = X_test[col].fillna(X_test[col].mean())\n",
    "\n",
    "    # 범주형/수치형 변수 구분\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "    # 범주형 변수 인코딩\n",
    "    ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    X[categorical_features] = ordinal_encoder.fit_transform(X[categorical_features])\n",
    "    X_test[categorical_features] = ordinal_encoder.transform(X_test[categorical_features])\n",
    "\n",
    "    # 수치형 변수 스케일링\n",
    "    scaler = MinMaxScaler()\n",
    "    X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "    # 이상치 처리\n",
    "    for col in numerical_features:\n",
    "        q1 = X[col].quantile(0.01)\n",
    "        q3 = X[col].quantile(0.99)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - (1.5 * iqr)\n",
    "        upper_bound = q3 + (1.5 * iqr)\n",
    "        X[col] = np.clip(X[col], lower_bound, upper_bound)\n",
    "        X_test[col] = np.clip(X_test[col], lower_bound, upper_bound)\n",
    "\n",
    "    # 파워 변환으로 분포 개선\n",
    "    power = PowerTransformer(method='yeo-johnson')\n",
    "    X[numerical_features] = power.fit_transform(X[numerical_features])\n",
    "    X_test[numerical_features] = power.transform(X_test[numerical_features])\n",
    "    \n",
    "    # 상호작용 특성을 저장할 리스트\n",
    "    interact_features = []\n",
    "    \n",
    "    # 상호작용 특성 생성\n",
    "    for i in range(min(10, len(numerical_features))):\n",
    "        for j in range(i+1, min(11, len(numerical_features))):\n",
    "            feat1, feat2 = numerical_features[i], numerical_features[j]\n",
    "            feat_name = f'{feat1}_{feat2}_interact'\n",
    "            interact_features.append(feat_name)\n",
    "            X[feat_name] = X[feat1] * X[feat2]\n",
    "            X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
    "    \n",
    "    # 중요: 범주형 변수는 float로 변환하지 않음\n",
    "    # 수치형 변수와 상호작용 특성만 float로 변환\n",
    "    X[numerical_features + interact_features] = X[numerical_features + interact_features].astype(float)\n",
    "    X_test[numerical_features + interact_features] = X_test[numerical_features + interact_features].astype(float)\n",
    "    \n",
    "    # 범주형 변수는 정수형으로 유지\n",
    "    X[categorical_features] = X[categorical_features].astype(int)\n",
    "    X_test[categorical_features] = X_test[categorical_features].astype(int)\n",
    "    \n",
    "    return X, y, X_test, sample_submission, categorical_features\n",
    "\n",
    "# CatBoost 최적 파라미터\n",
    "def get_cat_params():\n",
    "    return {\n",
    "        \"iterations\": 1500,      # 3000 -> 1500 (효율성 개선)\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"depth\": 8,\n",
    "        \"l2_leaf_reg\": 5,\n",
    "        \"border_count\": 128,\n",
    "        \"subsample\": 0.8,\n",
    "        \"random_strength\": 0.5,\n",
    "        \"bagging_temperature\": 1,\n",
    "        \"od_type\": \"Iter\",\n",
    "        \"od_wait\": 50,\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"verbose\": 100,\n",
    "        \"random_seed\": 42,\n",
    "        \"class_weights\": [1, 3]  # 클래스 가중치 추가 (3:1 불균형 비율)\n",
    "    }\n",
    "\n",
    "# LightGBM 최적 파라미터\n",
    "def get_lgb_params():\n",
    "    return {\n",
    "        \"n_estimators\": 1500,    # 3000 -> 1500 (효율성 개선)\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"num_leaves\": 64,\n",
    "        \"max_depth\": 12,\n",
    "        \"min_data_in_leaf\": 20,\n",
    "        \"max_bin\": 255,\n",
    "        \"subsample\": 0.8,\n",
    "        \"subsample_freq\": 1,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"min_child_weight\": 0.001,\n",
    "        \"reg_alpha\": 5,\n",
    "        \"reg_lambda\": 10,\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"verbose\": -1,\n",
    "        \"random_state\": 42,\n",
    "        \"scale_pos_weight\": 3.0  # 불균형 처리 (3:1 불균형 비율)\n",
    "    }\n",
    "\n",
    "# XGBoost 최적 파라미터\n",
    "def get_xgb_params():\n",
    "    return {\n",
    "        \"n_estimators\": 1500,    # 3000 -> 1500 (효율성 개선)\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 10,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"gamma\": 0.1,\n",
    "        \"reg_alpha\": 5,\n",
    "        \"reg_lambda\": 10,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"gpu_hist\",  # GPU 가속화 (사용 가능한 경우)\n",
    "        \"random_state\": 42,\n",
    "        \"scale_pos_weight\": 3.0  # 불균형 처리 (3:1 불균형 비율)\n",
    "    }\n",
    "\n",
    "# 가중치 최적화 함수 (예외 처리 추가)\n",
    "def optimize_weights(predictions, y_true):\n",
    "    def objective(weights):\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / np.sum(weights)  # 정규화\n",
    "        weighted_pred = np.sum([w * p for w, p in zip(weights, predictions)], axis=0)\n",
    "        return -roc_auc_score(y_true, weighted_pred)  # 최대화를 위해 음수 사용\n",
    "    \n",
    "    n_models = len(predictions)\n",
    "    # 기본 가중치를 모델 성능 기반으로 초기화\n",
    "    initial_aucs = [roc_auc_score(y_true, pred) for pred in predictions]\n",
    "    initial_weights = np.array(initial_aucs) / sum(initial_aucs)\n",
    "    \n",
    "    bounds = [(0, 1) for _ in range(n_models)]\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    \n",
    "    try:\n",
    "        result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        return result.x\n",
    "    except Exception as e:\n",
    "        print(f\"최적화 실패, 기본 가중치 사용: {e}\")\n",
    "        return initial_weights\n",
    "\n",
    "# 배치 예측 함수 (메모리 효율성)\n",
    "def batch_predict(model, X, batch_size=10000):\n",
    "    predictions = []\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch_pred = model.predict_proba(X.iloc[i:i+batch_size])[:, 1]\n",
    "        predictions.append(batch_pred)\n",
    "    return np.concatenate(predictions)\n",
    "\n",
    "# 모델 훈련 및 예측 함수\n",
    "def train_and_predict():\n",
    "    # 데이터 로드 및 전처리\n",
    "    X, y, X_test, sample_submission, categorical_features = preprocess_data(\n",
    "        'train.csv',\n",
    "        'test.csv'\n",
    "    )\n",
    "    \n",
    "    n_splits = 5\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # 모델 파라미터\n",
    "    cat_params = get_cat_params()\n",
    "    lgb_params = get_lgb_params()\n",
    "    xgb_params = get_xgb_params()\n",
    "    \n",
    "    # 예측 결과 저장 (3개 모델)\n",
    "    oof_preds_cat = np.zeros(len(X))\n",
    "    oof_preds_lgb = np.zeros(len(X))\n",
    "    oof_preds_xgb = np.zeros(len(X))\n",
    "    test_preds_cat = np.zeros(len(X_test))\n",
    "    test_preds_lgb = np.zeros(len(X_test))\n",
    "    test_preds_xgb = np.zeros(len(X_test))\n",
    "    \n",
    "    # 각 폴드별 최고 성능 모델 저장\n",
    "    best_models = {'cat': None, 'lgb': None, 'xgb': None}\n",
    "    best_score = 0\n",
    "    \n",
    "    # K-Fold 훈련\n",
    "    for fold_idx, (tr_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\n==== Fold {fold_idx}/{n_splits} ====\")\n",
    "        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "        \n",
    "        try:\n",
    "            # RandomUnderSampler 사용\n",
    "            rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)  \n",
    "            X_tr_res, y_tr_res = rus.fit_resample(X_tr, y_tr)\n",
    "            \n",
    "            # CatBoost 학습\n",
    "            print(\"Training CatBoost...\")\n",
    "            cat_model = CatBoostClassifier(**cat_params)\n",
    "            \n",
    "            cat_train_pool = Pool(X_tr_res, y_tr_res, cat_features=categorical_features)\n",
    "            cat_val_pool = Pool(X_val, y_val, cat_features=categorical_features)\n",
    "            \n",
    "            cat_model.fit(\n",
    "                cat_train_pool, \n",
    "                eval_set=cat_val_pool,\n",
    "                early_stopping_rounds=100,\n",
    "                verbose=200\n",
    "            )\n",
    "            \n",
    "            # LightGBM 학습\n",
    "            print(\"Training LightGBM...\")\n",
    "            lgb_model = LGBMClassifier(**lgb_params)\n",
    "            lgb_model.fit(\n",
    "                X_tr_res, y_tr_res,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric='auc',\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=True)]\n",
    "            )\n",
    "            \n",
    "            # XGBoost 학습\n",
    "            print(\"Training XGBoost...\")\n",
    "            xgb_model = XGBClassifier(**xgb_params)\n",
    "            xgb_model.fit(\n",
    "                X_tr_res, y_tr_res,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric='auc',\n",
    "                early_stopping_rounds=100,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # 개별 모델 검증 세트 예측\n",
    "            cat_val_pred = cat_model.predict_proba(X_val)[:, 1]\n",
    "            lgb_val_pred = lgb_model.predict_proba(X_val)[:, 1]\n",
    "            xgb_val_pred = xgb_model.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            # OOF 예측 저장\n",
    "            oof_preds_cat[val_idx] = cat_val_pred\n",
    "            oof_preds_lgb[val_idx] = lgb_val_pred\n",
    "            oof_preds_xgb[val_idx] = xgb_val_pred\n",
    "            \n",
    "            # 테스트 세트 예측\n",
    "            test_preds_cat += batch_predict(cat_model, X_test) / n_splits\n",
    "            test_preds_lgb += batch_predict(lgb_model, X_test) / n_splits\n",
    "            test_preds_xgb += batch_predict(xgb_model, X_test) / n_splits\n",
    "            \n",
    "            # 개별 모델 성능 확인\n",
    "            cat_auc = roc_auc_score(y_val, cat_val_pred)\n",
    "            lgb_auc = roc_auc_score(y_val, lgb_val_pred)\n",
    "            xgb_auc = roc_auc_score(y_val, xgb_val_pred)\n",
    "            print(f\"CatBoost Fold {fold_idx} AUC: {cat_auc:.6f}\")\n",
    "            print(f\"LightGBM Fold {fold_idx} AUC: {lgb_auc:.6f}\")\n",
    "            print(f\"XGBoost Fold {fold_idx} AUC: {xgb_auc:.6f}\")\n",
    "            \n",
    "            # 가중치 최적화로 앙상블\n",
    "            weights = optimize_weights(\n",
    "                [cat_val_pred, lgb_val_pred, xgb_val_pred], \n",
    "                y_val\n",
    "            )\n",
    "            weighted_val_pred = (\n",
    "                weights[0] * cat_val_pred + \n",
    "                weights[1] * lgb_val_pred + \n",
    "                weights[2] * xgb_val_pred\n",
    "            )\n",
    "            ensemble_auc = roc_auc_score(y_val, weighted_val_pred)\n",
    "            print(f\"Ensemble Fold {fold_idx} AUC: {ensemble_auc:.6f} (weights: {weights})\")\n",
    "            \n",
    "            # 최고 성능 모델 업데이트\n",
    "            if ensemble_auc > best_score:\n",
    "                best_score = ensemble_auc\n",
    "                best_models['cat'] = cat_model\n",
    "                best_models['lgb'] = lgb_model\n",
    "                best_models['xgb'] = xgb_model\n",
    "                print(f\"New best model found! Score: {best_score:.6f}\")\n",
    "                \n",
    "                # 모델 저장\n",
    "                best_models['cat'].save_model(f'best_cat_model_fold{fold_idx}.cbm')\n",
    "                import joblib\n",
    "                joblib.dump(best_models['lgb'], f'best_lgb_model_fold{fold_idx}.bin')\n",
    "                joblib.dump(best_models['xgb'], f'best_xgb_model_fold{fold_idx}.bin')\n",
    "            \n",
    "            # 모델 훈련 진행 확인을 위한 로깅 추가\n",
    "            print(f\"훈련 데이터 크기: {len(X_tr)} -> 리샘플링 후: {len(X_tr_res)}\")\n",
    "            print(f\"훈련 데이터 클래스 분포: {np.bincount(y_tr)}\")\n",
    "            print(f\"리샘플링 후 클래스 분포: {np.bincount(y_tr_res)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fold {fold_idx}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # 메모리 정리\n",
    "        gc.collect()\n",
    "    \n",
    "    # 전체 OOF 성능 평가\n",
    "    cat_oof_auc = roc_auc_score(y, oof_preds_cat)\n",
    "    lgb_oof_auc = roc_auc_score(y, oof_preds_lgb)\n",
    "    xgb_oof_auc = roc_auc_score(y, oof_preds_xgb)\n",
    "    print(f\"\\nCatBoost OOF AUC: {cat_oof_auc:.6f}\")\n",
    "    print(f\"LightGBM OOF AUC: {lgb_oof_auc:.6f}\")\n",
    "    print(f\"XGBoost OOF AUC: {xgb_oof_auc:.6f}\")\n",
    "    \n",
    "    # 전체 데이터에 대한 최적 가중치 계산\n",
    "    final_weights = optimize_weights(\n",
    "        [oof_preds_cat, oof_preds_lgb, oof_preds_xgb], \n",
    "        y\n",
    "    )\n",
    "    oof_ensemble = (\n",
    "        final_weights[0] * oof_preds_cat + \n",
    "        final_weights[1] * oof_preds_lgb + \n",
    "        final_weights[2] * oof_preds_xgb\n",
    "    )\n",
    "    ensemble_oof_auc = roc_auc_score(y, oof_ensemble)\n",
    "    print(f\"Final Ensemble OOF AUC: {ensemble_oof_auc:.6f}\")\n",
    "    print(f\"Final weights: CatBoost={final_weights[0]:.4f}, LightGBM={final_weights[1]:.4f}, XGBoost={final_weights[2]:.4f}\")\n",
    "    \n",
    "    # 테스트 데이터 최종 예측\n",
    "    final_prediction = (\n",
    "        final_weights[0] * test_preds_cat + \n",
    "        final_weights[1] * test_preds_lgb + \n",
    "        final_weights[2] * test_preds_xgb\n",
    "    )\n",
    "    \n",
    "    # 특성 중요도 시각화 (LightGBM 기준)\n",
    "    if best_models['lgb'] is not None:\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': best_models['lgb'].feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "        plt.title('LightGBM Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png')\n",
    "        print(\"Feature importance plot saved.\")\n",
    "    \n",
    "    # 제출 파일 생성\n",
    "    sample_submission['probability'] = final_prediction\n",
    "    submission_path = \"cat_lgb_xgb_ensemble.csv\"\n",
    "    sample_submission.to_csv(submission_path, index=False)\n",
    "    print(f\"\\nSubmission saved: {submission_path}\")\n",
    "    print(f\"Final Ensemble OOF AUC: {ensemble_oof_auc:.6f}\")\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
