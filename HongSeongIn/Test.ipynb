{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feat_name] = X[feat1] * X[feat2]\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test[feat_name] = X_test[feat1] * X_test[feat2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Fold 1/5 ====\n",
      "Training CatBoost...\n",
      "0:\ttest: 0.7189962\tbest: 0.7189962 (0)\ttotal: 671ms\tremaining: 16m 45s\n",
      "200:\ttest: 0.7358459\tbest: 0.7358459 (200)\ttotal: 1m 42s\tremaining: 11m 5s\n",
      "400:\ttest: 0.7365983\tbest: 0.7366020 (392)\ttotal: 2m 58s\tremaining: 8m 9s\n",
      "600:\ttest: 0.7368940\tbest: 0.7368940 (600)\ttotal: 4m 12s\tremaining: 6m 17s\n",
      "800:\ttest: 0.7371592\tbest: 0.7371750 (763)\ttotal: 5m 59s\tremaining: 5m 13s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.737193509\n",
      "bestIteration = 882\n",
      "\n",
      "Shrink model to first 883 iterations.\n",
      "Training LightGBM...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[341]\tvalid_0's auc: 0.737037\n",
      "CatBoost Fold 1 AUC: 0.737194\n",
      "LightGBM Fold 1 AUC: 0.737037\n",
      "Ensemble Fold 1 AUC: 0.737817 (weights: [0.50018356 0.49981644])\n",
      "New best model found! Score: 0.737817\n",
      "훈련 데이터 크기: 205080 -> 리샘플링 후: 158946\n",
      "훈련 데이터 클래스 분포: [152098  52982]\n",
      "리샘플링 후 클래스 분포: [105964  52982]\n",
      "\n",
      "==== Fold 2/5 ====\n",
      "Training CatBoost...\n",
      "0:\ttest: 0.7193260\tbest: 0.7193260 (0)\ttotal: 620ms\tremaining: 15m 28s\n",
      "200:\ttest: 0.7396230\tbest: 0.7396230 (200)\ttotal: 1m 42s\tremaining: 11m\n",
      "400:\ttest: 0.7408615\tbest: 0.7408620 (398)\ttotal: 3m 4s\tremaining: 8m 24s\n",
      "600:\ttest: 0.7413237\tbest: 0.7413237 (600)\ttotal: 4m 21s\tremaining: 6m 31s\n",
      "800:\ttest: 0.7419652\tbest: 0.7419689 (795)\ttotal: 6m 36s\tremaining: 5m 45s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7420514707\n",
      "bestIteration = 878\n",
      "\n",
      "Shrink model to first 879 iterations.\n",
      "Training LightGBM...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[412]\tvalid_0's auc: 0.742653\n",
      "CatBoost Fold 2 AUC: 0.742051\n",
      "LightGBM Fold 2 AUC: 0.742653\n",
      "Ensemble Fold 2 AUC: 0.743090 (weights: [0.49979731 0.50020269])\n",
      "New best model found! Score: 0.743090\n",
      "훈련 데이터 크기: 205081 -> 리샘플링 후: 158949\n",
      "훈련 데이터 클래스 분포: [152098  52983]\n",
      "리샘플링 후 클래스 분포: [105966  52983]\n",
      "\n",
      "==== Fold 3/5 ====\n",
      "Training CatBoost...\n",
      "0:\ttest: 0.7239672\tbest: 0.7239672 (0)\ttotal: 695ms\tremaining: 17m 22s\n",
      "200:\ttest: 0.7374320\tbest: 0.7374320 (200)\ttotal: 2m 7s\tremaining: 13m 44s\n",
      "400:\ttest: 0.7383240\tbest: 0.7383321 (397)\ttotal: 3m 56s\tremaining: 10m 48s\n",
      "600:\ttest: 0.7387220\tbest: 0.7387220 (600)\ttotal: 5m 42s\tremaining: 8m 32s\n",
      "800:\ttest: 0.7391793\tbest: 0.7391835 (795)\ttotal: 8m 5s\tremaining: 7m 3s\n",
      "1000:\ttest: 0.7393290\tbest: 0.7393315 (998)\ttotal: 10m 41s\tremaining: 5m 19s\n",
      "1200:\ttest: 0.7393771\tbest: 0.7393872 (1191)\ttotal: 13m 17s\tremaining: 3m 18s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7394117835\n",
      "bestIteration = 1261\n",
      "\n",
      "Shrink model to first 1262 iterations.\n",
      "Training LightGBM...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[337]\tvalid_0's auc: 0.740182\n",
      "CatBoost Fold 3 AUC: 0.739412\n",
      "LightGBM Fold 3 AUC: 0.740182\n",
      "Ensemble Fold 3 AUC: 0.740437 (weights: [0.4997398 0.5002602])\n",
      "훈련 데이터 크기: 205081 -> 리샘플링 후: 158949\n",
      "훈련 데이터 클래스 분포: [152098  52983]\n",
      "리샘플링 후 클래스 분포: [105966  52983]\n",
      "\n",
      "==== Fold 4/5 ====\n",
      "Training CatBoost...\n",
      "0:\ttest: 0.7218018\tbest: 0.7218018 (0)\ttotal: 520ms\tremaining: 12m 59s\n",
      "200:\ttest: 0.7366983\tbest: 0.7366983 (200)\ttotal: 2m 12s\tremaining: 14m 18s\n",
      "400:\ttest: 0.7372907\tbest: 0.7372917 (399)\ttotal: 4m 9s\tremaining: 11m 23s\n",
      "600:\ttest: 0.7374743\tbest: 0.7374750 (599)\ttotal: 5m 57s\tremaining: 8m 55s\n",
      "800:\ttest: 0.7375752\tbest: 0.7375846 (731)\ttotal: 8m 24s\tremaining: 7m 20s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7375846142\n",
      "bestIteration = 731\n",
      "\n",
      "Shrink model to first 732 iterations.\n",
      "Training LightGBM...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[258]\tvalid_0's auc: 0.737454\n",
      "CatBoost Fold 4 AUC: 0.737585\n",
      "LightGBM Fold 4 AUC: 0.737454\n",
      "Ensemble Fold 4 AUC: 0.738020 (weights: [0.50004439 0.49995561])\n",
      "훈련 데이터 크기: 205081 -> 리샘플링 후: 158946\n",
      "훈련 데이터 클래스 분포: [152099  52982]\n",
      "리샘플링 후 클래스 분포: [105964  52982]\n",
      "\n",
      "==== Fold 5/5 ====\n",
      "Training CatBoost...\n",
      "0:\ttest: 0.7242508\tbest: 0.7242508 (0)\ttotal: 588ms\tremaining: 14m 40s\n",
      "200:\ttest: 0.7370819\tbest: 0.7370819 (200)\ttotal: 2m 5s\tremaining: 13m 28s\n",
      "400:\ttest: 0.7382863\tbest: 0.7382873 (396)\ttotal: 3m 51s\tremaining: 10m 33s\n",
      "600:\ttest: 0.7387511\tbest: 0.7387511 (600)\ttotal: 5m 32s\tremaining: 8m 17s\n",
      "800:\ttest: 0.7391901\tbest: 0.7391914 (785)\ttotal: 7m 41s\tremaining: 6m 42s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7392571118\n",
      "bestIteration = 879\n",
      "\n",
      "Shrink model to first 880 iterations.\n",
      "Training LightGBM...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[353]\tvalid_0's auc: 0.740742\n",
      "CatBoost Fold 5 AUC: 0.739257\n",
      "LightGBM Fold 5 AUC: 0.740742\n",
      "Ensemble Fold 5 AUC: 0.740691 (weights: [0.49942557 0.50057443])\n",
      "훈련 데이터 크기: 205081 -> 리샘플링 후: 158946\n",
      "훈련 데이터 클래스 분포: [152099  52982]\n",
      "리샘플링 후 클래스 분포: [105964  52982]\n",
      "\n",
      "CatBoost OOF AUC: 0.739084\n",
      "LightGBM OOF AUC: 0.739604\n",
      "Final Ensemble OOF AUC: 0.740011\n",
      "Final weights: CatBoost=0.4998, LightGBM=0.5002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 49884 (\\N{HANGUL SYLLABLE SI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 49696 (\\N{HANGUL SYLLABLE SUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 44592 (\\N{HANGUL SYLLABLE GI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 53076 (\\N{HANGUL SYLLABLE KO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 46300 (\\N{HANGUL SYLLABLE DEU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 51665 (\\N{HANGUL SYLLABLE JIB}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 46108 (\\N{HANGUL SYLLABLE DOEN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 49888 (\\N{HANGUL SYLLABLE SIN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 49440 (\\N{HANGUL SYLLABLE SEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 45212 (\\N{HANGUL SYLLABLE NAN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 51088 (\\N{HANGUL SYLLABLE JA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 52509 (\\N{HANGUL SYLLABLE CONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 49373 (\\N{HANGUL SYLLABLE SAENG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 49457 (\\N{HANGUL SYLLABLE SEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 48176 (\\N{HANGUL SYLLABLE BAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 50500 (\\N{HANGUL SYLLABLE A}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 45817 (\\N{HANGUL SYLLABLE DANG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 45208 (\\N{HANGUL SYLLABLE NA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 49885 (\\N{HANGUL SYLLABLE SIG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 44221 (\\N{HANGUL SYLLABLE GYEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 44284 (\\N{HANGUL SYLLABLE GWA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 51068 (\\N{HANGUL SYLLABLE IL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 54943 (\\N{HANGUL SYLLABLE HOES}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 53364 (\\N{HANGUL SYLLABLE KEUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 47532 (\\N{HANGUL SYLLABLE RI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 45769 (\\N{HANGUL SYLLABLE NIG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 45236 (\\N{HANGUL SYLLABLE NAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 48120 (\\N{HANGUL SYLLABLE MI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 49464 (\\N{HANGUL SYLLABLE SE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 51452 (\\N{HANGUL SYLLABLE JU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 51077 (\\N{HANGUL SYLLABLE IB}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 51200 (\\N{HANGUL SYLLABLE JEO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 51109 (\\N{HANGUL SYLLABLE JANG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 54028 (\\N{HANGUL SYLLABLE PA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 53944 (\\N{HANGUL SYLLABLE TEU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 45320 (\\N{HANGUL SYLLABLE NEO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 50752 (\\N{HANGUL SYLLABLE WA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 54844 (\\N{HANGUL SYLLABLE HON}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 54633 (\\N{HANGUL SYLLABLE HAB}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 50640 (\\N{HANGUL SYLLABLE E}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 49436 (\\N{HANGUL SYLLABLE SEO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 54644 (\\N{HANGUL SYLLABLE HAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 46041 (\\N{HANGUL SYLLABLE DONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 51076 (\\N{HANGUL SYLLABLE IM}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 51613 (\\N{HANGUL SYLLABLE JEUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 53945 (\\N{HANGUL SYLLABLE TEUG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 50976 (\\N{HANGUL SYLLABLE YU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 54805 (\\N{HANGUL SYLLABLE HYEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 48520 (\\N{HANGUL SYLLABLE BUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 50896 (\\N{HANGUL SYLLABLE WEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 51064 (\\N{HANGUL SYLLABLE IN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 44288 (\\N{HANGUL SYLLABLE GWAN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 51656 (\\N{HANGUL SYLLABLE JIL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:323: UserWarning: Glyph 54872 (\\N{HANGUL SYLLABLE HWAN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 49884 (\\N{HANGUL SYLLABLE SI}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 49696 (\\N{HANGUL SYLLABLE SUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 44592 (\\N{HANGUL SYLLABLE GI}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 53076 (\\N{HANGUL SYLLABLE KO}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 46300 (\\N{HANGUL SYLLABLE DEU}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 51665 (\\N{HANGUL SYLLABLE JIB}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 46108 (\\N{HANGUL SYLLABLE DOEN}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 49888 (\\N{HANGUL SYLLABLE SIN}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 49440 (\\N{HANGUL SYLLABLE SEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 45212 (\\N{HANGUL SYLLABLE NAN}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 51088 (\\N{HANGUL SYLLABLE JA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 52509 (\\N{HANGUL SYLLABLE CONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 49373 (\\N{HANGUL SYLLABLE SAENG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 49457 (\\N{HANGUL SYLLABLE SEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 48176 (\\N{HANGUL SYLLABLE BAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 50500 (\\N{HANGUL SYLLABLE A}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 45817 (\\N{HANGUL SYLLABLE DANG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 45208 (\\N{HANGUL SYLLABLE NA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 49885 (\\N{HANGUL SYLLABLE SIG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 44221 (\\N{HANGUL SYLLABLE GYEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 44284 (\\N{HANGUL SYLLABLE GWA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 51068 (\\N{HANGUL SYLLABLE IL}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 54943 (\\N{HANGUL SYLLABLE HOES}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 53364 (\\N{HANGUL SYLLABLE KEUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 47532 (\\N{HANGUL SYLLABLE RI}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 45769 (\\N{HANGUL SYLLABLE NIG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 45236 (\\N{HANGUL SYLLABLE NAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 48120 (\\N{HANGUL SYLLABLE MI}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 49464 (\\N{HANGUL SYLLABLE SE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 51452 (\\N{HANGUL SYLLABLE JU}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 51077 (\\N{HANGUL SYLLABLE IB}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 51200 (\\N{HANGUL SYLLABLE JEO}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 51109 (\\N{HANGUL SYLLABLE JANG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 54028 (\\N{HANGUL SYLLABLE PA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 53944 (\\N{HANGUL SYLLABLE TEU}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 45320 (\\N{HANGUL SYLLABLE NEO}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 50752 (\\N{HANGUL SYLLABLE WA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 54844 (\\N{HANGUL SYLLABLE HON}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 54633 (\\N{HANGUL SYLLABLE HAB}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 50640 (\\N{HANGUL SYLLABLE E}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 49436 (\\N{HANGUL SYLLABLE SEO}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 54644 (\\N{HANGUL SYLLABLE HAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 46041 (\\N{HANGUL SYLLABLE DONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 51076 (\\N{HANGUL SYLLABLE IM}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 51613 (\\N{HANGUL SYLLABLE JEUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 53945 (\\N{HANGUL SYLLABLE TEUG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 50976 (\\N{HANGUL SYLLABLE YU}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 54805 (\\N{HANGUL SYLLABLE HYEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 48520 (\\N{HANGUL SYLLABLE BUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 50896 (\\N{HANGUL SYLLABLE WEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 51064 (\\N{HANGUL SYLLABLE IN}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 44288 (\\N{HANGUL SYLLABLE GWAN}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 51656 (\\N{HANGUL SYLLABLE JIL}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n",
      "C:\\Users\\tjddl\\AppData\\Local\\Temp\\ipykernel_17464\\3687301400.py:324: UserWarning: Glyph 54872 (\\N{HANGUL SYLLABLE HWAN}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('feature_importance.png')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance plot saved.\n",
      "\n",
      "Submission saved: cat_lgb_ensemble.csv\n",
      "Final Ensemble OOF AUC: 0.740011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49884 (\\N{HANGUL SYLLABLE SI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49696 (\\N{HANGUL SYLLABLE SUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44592 (\\N{HANGUL SYLLABLE GI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 53076 (\\N{HANGUL SYLLABLE KO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 46300 (\\N{HANGUL SYLLABLE DEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51665 (\\N{HANGUL SYLLABLE JIB}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 46108 (\\N{HANGUL SYLLABLE DOEN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49888 (\\N{HANGUL SYLLABLE SIN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49440 (\\N{HANGUL SYLLABLE SEON}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45212 (\\N{HANGUL SYLLABLE NAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51088 (\\N{HANGUL SYLLABLE JA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 52509 (\\N{HANGUL SYLLABLE CONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49373 (\\N{HANGUL SYLLABLE SAENG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49457 (\\N{HANGUL SYLLABLE SEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 48176 (\\N{HANGUL SYLLABLE BAE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50500 (\\N{HANGUL SYLLABLE A}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45817 (\\N{HANGUL SYLLABLE DANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45208 (\\N{HANGUL SYLLABLE NA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49885 (\\N{HANGUL SYLLABLE SIG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44221 (\\N{HANGUL SYLLABLE GYEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44284 (\\N{HANGUL SYLLABLE GWA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51068 (\\N{HANGUL SYLLABLE IL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54943 (\\N{HANGUL SYLLABLE HOES}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 53364 (\\N{HANGUL SYLLABLE KEUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 47532 (\\N{HANGUL SYLLABLE RI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45769 (\\N{HANGUL SYLLABLE NIG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45236 (\\N{HANGUL SYLLABLE NAE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 48120 (\\N{HANGUL SYLLABLE MI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49464 (\\N{HANGUL SYLLABLE SE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51452 (\\N{HANGUL SYLLABLE JU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51077 (\\N{HANGUL SYLLABLE IB}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51200 (\\N{HANGUL SYLLABLE JEO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51109 (\\N{HANGUL SYLLABLE JANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54028 (\\N{HANGUL SYLLABLE PA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 53944 (\\N{HANGUL SYLLABLE TEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45320 (\\N{HANGUL SYLLABLE NEO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50752 (\\N{HANGUL SYLLABLE WA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54844 (\\N{HANGUL SYLLABLE HON}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54633 (\\N{HANGUL SYLLABLE HAB}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50640 (\\N{HANGUL SYLLABLE E}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49436 (\\N{HANGUL SYLLABLE SEO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54644 (\\N{HANGUL SYLLABLE HAE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 46041 (\\N{HANGUL SYLLABLE DONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51076 (\\N{HANGUL SYLLABLE IM}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51613 (\\N{HANGUL SYLLABLE JEUNG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 53945 (\\N{HANGUL SYLLABLE TEUG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50976 (\\N{HANGUL SYLLABLE YU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54805 (\\N{HANGUL SYLLABLE HYEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 48520 (\\N{HANGUL SYLLABLE BUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50896 (\\N{HANGUL SYLLABLE WEON}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51064 (\\N{HANGUL SYLLABLE IN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44288 (\\N{HANGUL SYLLABLE GWAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51656 (\\N{HANGUL SYLLABLE JIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\tjddl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54872 (\\N{HANGUL SYLLABLE HWAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pandas >= 1.3.0\n",
    "# numpy >= 1.20.0\n",
    "# matplotlib >= 3.4.0\n",
    "# seaborn >= 0.11.0\n",
    "# scikit-learn >= 1.0.0\n",
    "# imbalanced-learn >= 0.8.0\n",
    "# catboost >= 1.0.0\n",
    "# lightgbm >= 3.3.0\n",
    "# scipy >= 1.7.0\n",
    "\n",
    "# 데이터 불균형 비율 약 3:1(190,123 : 66,228)\n",
    "# 모델 파라미터 최적화\n",
    "# 모델 앙상블 구현\n",
    "# 최적 가중치 최적화\n",
    "# 최적 모델 저장\n",
    "# 최적 파라미터 출력\n",
    "# LightGBM 모델\n",
    "# CatBoost 모델 \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, PowerTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 데이터 불러오기 및 전처리\n",
    "def preprocess_data(train_path, test_path):\n",
    "    print(\"Loading data...\")\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    sample_submission = pd.read_csv(test_path.replace('test.csv', 'sample_submission.csv'))\n",
    "    \n",
    "    # ID 컬럼 제거\n",
    "    if 'ID' in train.columns:\n",
    "        train.drop(columns=['ID'], inplace=True)\n",
    "    if 'ID' in test.columns:\n",
    "        test.drop(columns=['ID'], inplace=True)\n",
    "\n",
    "    y = train['임신 성공 여부']\n",
    "    X = train.drop(columns=['임신 성공 여부'])\n",
    "    X_test = test.copy()\n",
    "\n",
    "    # 결측치 처리\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            X[col] = X[col].fillna('Unknown')\n",
    "            X_test[col] = X_test[col].fillna('Unknown')\n",
    "        else:\n",
    "            X[col] = X[col].fillna(X[col].mean())\n",
    "            X_test[col] = X_test[col].fillna(X[col].mean())\n",
    "\n",
    "    # 범주형/수치형 변수 구분\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "    # 범주형 변수 인코딩\n",
    "    ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    X[categorical_features] = ordinal_encoder.fit_transform(X[categorical_features])\n",
    "    X_test[categorical_features] = ordinal_encoder.transform(X_test[categorical_features])\n",
    "\n",
    "    # 수치형 변수 스케일링\n",
    "    scaler = MinMaxScaler()\n",
    "    X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "    X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "    # 이상치 처리\n",
    "    for col in numerical_features:\n",
    "        q1 = X[col].quantile(0.01)\n",
    "        q3 = X[col].quantile(0.99)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - (1.5 * iqr)\n",
    "        upper_bound = q3 + (1.5 * iqr)\n",
    "        X[col] = np.clip(X[col], lower_bound, upper_bound)\n",
    "        X_test[col] = np.clip(X_test[col], lower_bound, upper_bound)\n",
    "\n",
    "    # 파워 변환으로 분포 개선\n",
    "    power = PowerTransformer(method='yeo-johnson')\n",
    "    X[numerical_features] = power.fit_transform(X[numerical_features])\n",
    "    X_test[numerical_features] = power.transform(X_test[numerical_features])\n",
    "    \n",
    "    # 상호작용 특성을 저장할 리스트\n",
    "    interact_features = []\n",
    "    \n",
    "    # 상호작용 특성 생성\n",
    "    for i in range(min(10, len(numerical_features))):\n",
    "        for j in range(i+1, min(11, len(numerical_features))):\n",
    "            feat1, feat2 = numerical_features[i], numerical_features[j]\n",
    "            feat_name = f'{feat1}_{feat2}_interact'\n",
    "            interact_features.append(feat_name)\n",
    "            X[feat_name] = X[feat1] * X[feat2]\n",
    "            X_test[feat_name] = X_test[feat1] * X_test[feat2]\n",
    "    \n",
    "    # 중요: 범주형 변수는 float로 변환하지 않음\n",
    "    # 수치형 변수와 상호작용 특성만 float로 변환\n",
    "    X[numerical_features + interact_features] = X[numerical_features + interact_features].astype(float)\n",
    "    X_test[numerical_features + interact_features] = X_test[numerical_features + interact_features].astype(float)\n",
    "    \n",
    "    # 범주형 변수는 정수형으로 유지\n",
    "    X[categorical_features] = X[categorical_features].astype(int)\n",
    "    X_test[categorical_features] = X_test[categorical_features].astype(int)\n",
    "    \n",
    "    return X, y, X_test, sample_submission, categorical_features\n",
    "\n",
    "# CatBoost 최적 파라미터\n",
    "# CatBoost 최적 파라미터\n",
    "def get_cat_params():\n",
    "    return {\n",
    "        \"iterations\": 1500,      # 3000 -> 1500 (효율성 개선)\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"depth\": 8,\n",
    "        \"l2_leaf_reg\": 5,\n",
    "        \"border_count\": 128,\n",
    "        \"subsample\": 0.8,\n",
    "        \"random_strength\": 0.5,\n",
    "        \"bagging_temperature\": 1,\n",
    "        \"od_type\": \"Iter\",\n",
    "        \"od_wait\": 40,           # 50 -> 40 (변경됨)\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"verbose\": 100,\n",
    "        \"random_seed\": 42,\n",
    "        \"class_weights\": [1, 2.5],  # [1, 3] -> [1, 2.5] (변경됨)\n",
    "        \"boosting_type\": \"Ordered\"  # 추가됨\n",
    "    }\n",
    "\n",
    "# LightGBM 최적 파라미터\n",
    "def get_lgb_params():\n",
    "    return {\n",
    "        \"n_estimators\": 1200,    # 1500 -> 1200 (변경됨)\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"num_leaves\": 64,\n",
    "        \"max_depth\": 12,\n",
    "        \"min_data_in_leaf\": 20,\n",
    "        \"max_bin\": 255,\n",
    "        \"subsample\": 0.8,\n",
    "        \"subsample_freq\": 1,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"min_child_weight\": 0.001,\n",
    "        \"reg_alpha\": 5,\n",
    "        \"reg_lambda\": 10,\n",
    "        \"objective\": \"binary\",   # cross_entropy도 시도해 볼 수 있음\n",
    "        \"metric\": \"auc\",\n",
    "        \"boosting_type\": \"gbdt\", # dart도 시도해 볼 수 있음\n",
    "        \"verbose\": -1,\n",
    "        \"random_state\": 42,\n",
    "        \"scale_pos_weight\": 2.5  # 3.0 -> 2.5 (변경됨)\n",
    "    }\n",
    "\n",
    "# 가중치 최적화 함수 (예외 처리 추가)\n",
    "def optimize_weights(predictions, y_true):\n",
    "    def objective(weights):\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / np.sum(weights)  # 정규화\n",
    "        weighted_pred = np.sum([w * p for w, p in zip(weights, predictions)], axis=0)\n",
    "        return -roc_auc_score(y_true, weighted_pred)  # 최대화를 위해 음수 사용\n",
    "    \n",
    "    n_models = len(predictions)\n",
    "    # 기본 가중치를 모델 성능 기반으로 초기화\n",
    "    initial_aucs = [roc_auc_score(y_true, pred) for pred in predictions]\n",
    "    initial_weights = np.array(initial_aucs) / sum(initial_aucs)\n",
    "    \n",
    "    bounds = [(0, 1) for _ in range(n_models)]\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    \n",
    "    try:\n",
    "        result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        return result.x\n",
    "    except Exception as e:\n",
    "        print(f\"최적화 실패, 기본 가중치 사용: {e}\")\n",
    "        return initial_weights\n",
    "\n",
    "# 배치 예측 함수 (메모리 효율성)\n",
    "def batch_predict(model, X, batch_size=10000):\n",
    "    predictions = []\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch_pred = model.predict_proba(X.iloc[i:i+batch_size])[:, 1]\n",
    "        predictions.append(batch_pred)\n",
    "    return np.concatenate(predictions)\n",
    "\n",
    "# 모델 훈련 및 예측 함수\n",
    "def train_and_predict():\n",
    "    # 데이터 로드 및 전처리\n",
    "    X, y, X_test, sample_submission, categorical_features = preprocess_data(\n",
    "        'train.csv',\n",
    "        'test.csv'\n",
    "    )\n",
    "    \n",
    "    n_splits = 5\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # 모델 파라미터\n",
    "    cat_params = get_cat_params()\n",
    "    lgb_params = get_lgb_params()\n",
    "    \n",
    "    # 예측 결과 저장\n",
    "    oof_preds_cat = np.zeros(len(X))\n",
    "    oof_preds_lgb = np.zeros(len(X))\n",
    "    test_preds_cat = np.zeros(len(X_test))\n",
    "    test_preds_lgb = np.zeros(len(X_test))\n",
    "    \n",
    "    # 각 폴드별 최고 성능 모델 저장\n",
    "    best_models = {'cat': None, 'lgb': None}\n",
    "    best_score = 0\n",
    "    \n",
    "    # K-Fold 훈련\n",
    "    for fold_idx, (tr_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\n==== Fold {fold_idx}/{n_splits} ====\")\n",
    "        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "        \n",
    "        try:\n",
    "            # RandomUnderSampler 사용 (SMOTETomek 대체)\n",
    "            # 0.5 비율은 소수:다수 = 1:2로 조정\n",
    "            rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)  \n",
    "            X_tr_res, y_tr_res = rus.fit_resample(X_tr, y_tr)\n",
    "            \n",
    "            # CatBoost 학습 - categorical_features를 직접 문자열로 전달\n",
    "            print(\"Training CatBoost...\")\n",
    "            cat_model = CatBoostClassifier(**cat_params)\n",
    "            \n",
    "            cat_train_pool = Pool(X_tr_res, y_tr_res, cat_features=categorical_features)\n",
    "            cat_val_pool = Pool(X_val, y_val, cat_features=categorical_features)\n",
    "            \n",
    "            cat_model.fit(\n",
    "                cat_train_pool, \n",
    "                eval_set=cat_val_pool,\n",
    "                early_stopping_rounds=100,\n",
    "                verbose=200\n",
    "            )\n",
    "            \n",
    "            # LightGBM 학습\n",
    "            print(\"Training LightGBM...\")\n",
    "            lgb_model = LGBMClassifier(**lgb_params)\n",
    "            lgb_model.fit(\n",
    "                X_tr_res, y_tr_res,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric='auc',\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=True)]\n",
    "            )\n",
    "            \n",
    "            # 검증 세트 예측\n",
    "            cat_val_pred = cat_model.predict_proba(X_val)[:, 1]\n",
    "            lgb_val_pred = lgb_model.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            # OOF 예측 저장\n",
    "            oof_preds_cat[val_idx] = cat_val_pred\n",
    "            oof_preds_lgb[val_idx] = lgb_val_pred\n",
    "            \n",
    "            # 테스트 세트 예측 (배치 처리)\n",
    "            test_preds_cat += batch_predict(cat_model, X_test) / n_splits\n",
    "            test_preds_lgb += batch_predict(lgb_model, X_test) / n_splits\n",
    "            \n",
    "            # 개별 모델 성능 확인\n",
    "            cat_auc = roc_auc_score(y_val, cat_val_pred)\n",
    "            lgb_auc = roc_auc_score(y_val, lgb_val_pred)\n",
    "            print(f\"CatBoost Fold {fold_idx} AUC: {cat_auc:.6f}\")\n",
    "            print(f\"LightGBM Fold {fold_idx} AUC: {lgb_auc:.6f}\")\n",
    "            \n",
    "            # 가중치 최적화로 앙상블\n",
    "            weights = optimize_weights([cat_val_pred, lgb_val_pred], y_val)\n",
    "            weighted_val_pred = weights[0] * cat_val_pred + weights[1] * lgb_val_pred\n",
    "            ensemble_auc = roc_auc_score(y_val, weighted_val_pred)\n",
    "            print(f\"Ensemble Fold {fold_idx} AUC: {ensemble_auc:.6f} (weights: {weights})\")\n",
    "            \n",
    "            # 최고 성능 모델 업데이트\n",
    "            if ensemble_auc > best_score:\n",
    "                best_score = ensemble_auc\n",
    "                best_models['cat'] = cat_model\n",
    "                best_models['lgb'] = lgb_model\n",
    "                print(f\"New best model found! Score: {best_score:.6f}\")\n",
    "                \n",
    "                # 모델 저장\n",
    "                best_models['cat'].save_model(f'best_cat_model.cbm')\n",
    "                import joblib\n",
    "                joblib.dump(best_models['lgb'], f'best_lgb_model.bin')\n",
    "                \n",
    "            # 모델 훈련 진행 확인을 위한 로깅 추가\n",
    "            print(f\"훈련 데이터 크기: {len(X_tr)} -> 리샘플링 후: {len(X_tr_res)}\")\n",
    "            print(f\"훈련 데이터 클래스 분포: {np.bincount(y_tr)}\")\n",
    "            print(f\"리샘플링 후 클래스 분포: {np.bincount(y_tr_res)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fold {fold_idx}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # 메모리 정리\n",
    "        gc.collect()\n",
    "    \n",
    "    # 전체 OOF 성능 평가\n",
    "    cat_oof_auc = roc_auc_score(y, oof_preds_cat)\n",
    "    lgb_oof_auc = roc_auc_score(y, oof_preds_lgb)\n",
    "    print(f\"\\nCatBoost OOF AUC: {cat_oof_auc:.6f}\")\n",
    "    print(f\"LightGBM OOF AUC: {lgb_oof_auc:.6f}\")\n",
    "    \n",
    "    # 전체 데이터에 대한 최적 가중치 계산\n",
    "    final_weights = optimize_weights([oof_preds_cat, oof_preds_lgb], y)\n",
    "    oof_ensemble = final_weights[0] * oof_preds_cat + final_weights[1] * oof_preds_lgb\n",
    "    ensemble_oof_auc = roc_auc_score(y, oof_ensemble)\n",
    "    print(f\"Final Ensemble OOF AUC: {ensemble_oof_auc:.6f}\")\n",
    "    print(f\"Final weights: CatBoost={final_weights[0]:.4f}, LightGBM={final_weights[1]:.4f}\")\n",
    "    \n",
    "    # 테스트 데이터 최종 예측\n",
    "    final_prediction = final_weights[0] * test_preds_cat + final_weights[1] * test_preds_lgb\n",
    "    \n",
    "    # 특성 중요도 시각화\n",
    "    if best_models['lgb'] is not None:\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': best_models['lgb'].feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "        plt.title('LightGBM Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png')\n",
    "        print(\"Feature importance plot saved.\")\n",
    "    \n",
    "    # 제출 파일 생성\n",
    "    sample_submission['probability'] = final_prediction\n",
    "    submission_path = \"cat_lgb_ensemble.csv\"\n",
    "    sample_submission.to_csv(submission_path, index=False)\n",
    "    print(f\"\\nSubmission saved: {submission_path}\")\n",
    "    print(f\"Final Ensemble OOF AUC: {ensemble_oof_auc:.6f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
